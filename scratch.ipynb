{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "import sys\n",
    "sys.path.append('/home/peili')\n",
    "from src.datasets import get_dataset_transformer, get_dataset_jepa\n",
    "from src.dataloaders import get_dataloader\n",
    "from src.encoders import get_encoder\n",
    "from src.models import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "print(torch.cuda.is_available())  # Verify CUDA is available\n",
    "print(torch.cuda.current_device())  # Should return 0 (mapped to device 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ECG files:   0%|          | 0/10344 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ECG files: 100%|██████████| 10344/10344 [00:28<00:00, 364.21it/s]\n",
      "Processing Records: 100%|██████████| 10292/10292 [00:01<00:00, 10114.63it/s]\n",
      "Combining Results: 100%|██████████| 10292/10292 [00:04<00:00, 2263.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid samples: 7\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'Ga'\n",
    "model_name = 'ecg-jepa'\n",
    "seg_method = 'fix'\n",
    "train_encoder = 'train'\n",
    "num_epochs = 10\n",
    "ckpt = None\n",
    "\n",
    "fixed= True if seg_method == 'fix' else False\n",
    "train_encoder = True if train_encoder == 'train' else False\n",
    "save_dir = f\"{dataset_name}_{model_name}_{seg_method}\" + \"_encoder_trained\" if train_encoder else ''\n",
    "os.makedirs(save_dir, exist_ok=True) \n",
    "\n",
    "if model_name == 'transformer':\n",
    "    train_dataset, test_dataset = get_dataset_transformer(dataset_name, fixed=fixed, window_size=10, overlap=5)\n",
    "elif model_name == 'ecg-jepa':\n",
    "    train_dataset, test_dataset = get_dataset_jepa(dataset_name, reload=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peili/ECG_JEPA/models.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_dir)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder, embed_dim = get_encoder(model_name, device, load=True)\n",
    "train_loader, test_loader = get_dataloader(model_name, train_dataset, test_dataset, encoder, device, train_encoder)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_labels = 7\n",
    "model = get_model(model_name, embed_dim, n_labels, device)\n",
    "\n",
    "if train_encoder:\n",
    "    assert encoder != None\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = True\n",
    "    optimizer = optim.Adam(list(encoder.parameters()) + list(model.parameters()), lr=0.001)\n",
    "else:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "if ckpt:\n",
    "    checkpoint = torch.load(ckpt)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 258/258 [06:01<00:00,  1.40s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.6772\n",
      "Model checkpoint saved at Ga_ecg-jepa_fix_encoder_trained/epoch_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 258/258 [06:00<00:00,  1.40s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.6561\n",
      "Model checkpoint saved at Ga_ecg-jepa_fix_encoder_trained/epoch_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 258/258 [05:59<00:00,  1.39s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.6422\n",
      "Model checkpoint saved at Ga_ecg-jepa_fix_encoder_trained/epoch_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 258/258 [05:59<00:00,  1.39s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.6240\n",
      "Model checkpoint saved at Ga_ecg-jepa_fix_encoder_trained/epoch_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 258/258 [06:01<00:00,  1.40s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.6177\n",
      "Model checkpoint saved at Ga_ecg-jepa_fix_encoder_trained/epoch_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 258/258 [06:04<00:00,  1.41s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.6256\n",
      "Model checkpoint saved at Ga_ecg-jepa_fix_encoder_trained/epoch_6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 258/258 [06:01<00:00,  1.40s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.6094\n",
      "Model checkpoint saved at Ga_ecg-jepa_fix_encoder_trained/epoch_7.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 258/258 [04:13<00:00,  1.02batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.6023\n",
      "Model checkpoint saved at Ga_ecg-jepa_fix_encoder_trained/epoch_8.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 258/258 [02:59<00:00,  1.44batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.6019\n",
      "Model checkpoint saved at Ga_ecg-jepa_fix_encoder_trained/epoch_9.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 258/258 [03:01<00:00,  1.43batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 1.5958\n",
      "Model checkpoint saved at Ga_ecg-jepa_fix_encoder_trained/epoch_10.pth\n",
      "Time taken: 3155.2359256744385 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'Ga'\n",
    "model_name = 'ecg-jepa'\n",
    "seg_method = 'fix'\n",
    "train_encoder = 'train'\n",
    "num_epochs = 10\n",
    "ckpt = None\n",
    "\n",
    "fixed= True if seg_method == 'fix' else False\n",
    "train_encoder = True if train_encoder == 'train' else False\n",
    "save_dir = f\"{dataset_name}_{model_name}_{seg_method}\" + \"_encoder_trained\" if train_encoder else ''\n",
    "\n",
    "start_time = time.time()\n",
    "training_metrics = {\"loss\": []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if train_encoder: encoder.train()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch'):\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if train_encoder:\n",
    "            features = encoder.representation(inputs)\n",
    "            outputs = model(features)\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    average_loss = running_loss / len(train_loader)\n",
    "    training_metrics[\"loss\"].append(average_loss)\n",
    "    print(f'Average Loss: {average_loss:.4f}')\n",
    "\n",
    "    checkpoint_path = f'{save_dir}/epoch_{epoch + 1}.pth'\n",
    "\n",
    "    if train_encoder:\n",
    "        torch.save({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"encoder\": encoder.state_dict(),\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": average_loss\n",
    "        }, checkpoint_path)\n",
    "    else:\n",
    "        torch.save({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": average_loss\n",
    "        }, checkpoint_path)\n",
    "\n",
    "    print(f'Model checkpoint saved at {checkpoint_path}')\n",
    "    torch.cuda.empty_cache()\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Time taken: {training_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 36.62%\n",
      "Macro-Averaged AUC: 0.5462\n",
      "Class 0: Sensitivity: 0.9164, Specificity: 0.2363\n",
      "Class 1: Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Class 2: Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Class 3: Sensitivity: 0.0046, Specificity: 0.9963\n",
      "Class 4: Sensitivity: 0.0267, Specificity: 0.9771\n",
      "Class 5: Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Class 6: Sensitivity: 0.2675, Specificity: 0.8647\n",
      "Macro-Averaged Sensitivity: 0.1736\n",
      "Macro-Averaged Specificity: 0.8678\n",
      "Test metrics saved to Ga_ecg-jepa_fix_encoder_trained/test_metrics.json\n",
      "Time taken: 26.18837881088257 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "encoder.eval()\n",
    "model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_scores = []\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "\n",
    "        if train_encoder:\n",
    "            features = encoder.representation(inputs)\n",
    "            outputs = model(features)\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_scores.extend(torch.softmax(outputs, dim=1).cpu().numpy())  # Probabilities for all classes\n",
    "    \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on test data: {accuracy:.2f}%')\n",
    "\n",
    "# Evaluation: Macro-averaged AUC, Sensitivity, and Specificity\n",
    "y_true_one_hot = np.zeros((len(y_true), y_scores[0].shape[0]))  # Convert to one-hot encoding\n",
    "y_true_one_hot[np.arange(len(y_true)), y_true] = 1\n",
    "\n",
    "# AUC calculation for multilabel (macro-averaged)\n",
    "auc = roc_auc_score(y_true_one_hot, np.array(y_scores), average='macro')\n",
    "print(f'Macro-Averaged AUC: {auc:.4f}')\n",
    "\n",
    "# Sensitivity and Specificity per class\n",
    "num_classes = y_scores[0].shape[0]\n",
    "\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    y_true_binary = y_true_one_hot[:, i]  # Binary ground truth for class i\n",
    "\n",
    "    # Correct binary predictions based on the highest probability class\n",
    "    y_pred_binary = (np.argmax(np.array(y_scores), axis=1) == i).astype(int)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true_binary, y_pred_binary).ravel()\n",
    "\n",
    "    # Class-specific metrics\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    sensitivities.append(sensitivity)\n",
    "    specificities.append(specificity)\n",
    "\n",
    "    print(f'Class {i}: Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}')\n",
    "\n",
    "# Macro-averaged metrics\n",
    "macro_sensitivity = np.mean(sensitivities)\n",
    "macro_specificity = np.mean(specificities)\n",
    "print(f'Macro-Averaged Sensitivity: {macro_sensitivity:.4f}')\n",
    "print(f'Macro-Averaged Specificity: {macro_specificity:.4f}')\n",
    "\n",
    "test_metrics = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"macro_auc\": auc,\n",
    "    \"macro_sensitivity\": macro_sensitivity,\n",
    "    \"macro_specificity\": macro_specificity,\n",
    "    \"class_sensitivities\": sensitivities,\n",
    "    \"class_specificities\": specificities,\n",
    "}\n",
    "\n",
    "metrics_path = f\"{save_dir}/test_metrics.json\"\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump(test_metrics, f)\n",
    "print(f\"Test metrics saved to {metrics_path}\")\n",
    "\n",
    "end_time = time.time()\n",
    "testing_time = end_time - start_time\n",
    "print(f\"Time taken: {testing_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
